{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colornet\n",
    "\n",
    "Today, colorization is done by hand in Photoshop, a picture can take up to one month to colorize. It requires extensive research. A face alone needs up to 20 layers of pink, green and blue shades to get it just right. But something changed this year when Amir Avni used neural networks to [troll the subreddit](http://www.whatimade.today/our-frst-reddit-bot-coloring-b-2/) [/r/Colorization](https://www.reddit.com/r/Colorization/) - a community where people colorize historical black and white images manually using Photoshop. They were astonished with Amirâ€™s deep learning bot - what could take up to a month of manual labour could now be done in just a few seconds.\n",
    "\n",
    "### Colorizing Black&White photos\n",
    "\n",
    "Fascinated by Amirâ€™s neural network, Emill reproduced it and documented the process in the famous blog post: [Colorizing B&W Photos with Neural Networks](https://blog.floydhub.com/colorizing-b-w-photos-with-neural-networks/). In this notebook we will reproduce Emil's work by using the Full Version of his experiments.\n",
    "\n",
    "![colorization](https://blog.floydhub.com/content/images/2018/06/woman_results-1-min.png)\n",
    "*The middle picture is done with our neural network and the picture to the right is the original color photo - Image from the [Blog](https://blog.floydhub.com/colorizing-b-w-photos-with-neural-networks/)*\n",
    "\n",
    "We will:\n",
    "- Preprocess the image data for this CV task\n",
    "- Build and train the `colornet` model using Keras and Tensorflow\n",
    "- Evaluate our model on the test set\n",
    "- Run the model on your own black&white and colored pictures!\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- To execute a code cell, click on the cell and press `Shift + Enter` (shortcut for Run).\n",
    "- To learn more about Workspaces, check out the [Getting Started Notebook](get_started_workspace.ipynb).\n",
    "- **Tip**: *Feel free to try this Notebook with your own data and on your own super awesome colorization task.*\n",
    "\n",
    "Now, let's get started! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it now!\n",
    "\n",
    "Test out the Emil's pretrained model. Run the code Cell below and enter a URL with your pic in the widget below. Have fun!ðŸŽ‰\n",
    "\n",
    "Here are some URLs for testing:\n",
    "\n",
    "- (man, colored) http://www.bolsamania.com/cine/wp-content/uploads/2017/03/26-2.jpg\n",
    "- (landscape, colored) https://cdn.pixabay.com/photo/2017/04/07/18/23/landscape-2211587_960_720.jpg\n",
    "- (lion, b&w) https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQTXYpHhz45gaDHPsNulPFotlc72i3MDv_1RoOcQjEQx3sX-dWj\n",
    "\n",
    "\n",
    "Note: \n",
    "- You can also consider to use URL of colored pictures, in this way you can fully test the colorization on new images.\n",
    "- The first prediction can take up to one minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model... (it could take a while)\n",
      "Model loaded!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efba72034eb24dec8ed58df3219abc85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='', description='URL', placeholder='Insert URL of a pic'), Button(descriptionâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Testing on url images\n",
    "from ipywidgets import interact_manual\n",
    "from ipywidgets import widgets\n",
    "from support import prediction_from_url, load_pretrained_model\n",
    "\n",
    "(model, inception) = load_pretrained_model('/floyd/input/colornet/models/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels.h5',\n",
    "                      '/floyd/input/colornet/models/color_tensorflow_real_mode_300.h5')\n",
    "\n",
    "def get_prediction(URL):\n",
    "    prediction_from_url(URL, model, inception)\n",
    "\n",
    "interact_manual(get_prediction, URL=widgets.Text(placeholder='Insert URL of a pic'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup\n",
    "\n",
    "Let's start by importing some packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import random\n",
    "import keras\n",
    "\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.applications.inception_resnet_v2 import preprocess_input\n",
    "\n",
    "from keras.applications.xception import Xception\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.callbacks import TensorBoard \n",
    "\n",
    "from keras.engine import Layer\n",
    "from keras.layers import Conv2D, UpSampling2D, InputLayer, Conv2DTranspose, Input, Reshape, merge, concatenate, Activation, Dense, Dropout, Flatten, BatchNormalization\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.core import RepeatVector, Permute\n",
    "\n",
    "from skimage.color import rgb2lab, lab2rgb, rgb2gray, gray2rgb\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imsave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Parameters\n",
    "\n",
    "We'll set the hyperparameters for training our model. If you understand what they mean, feel free to play around - otherwise, we recommend keeping the defaults for your first run ðŸ™‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparams if GPU is available\n",
    "if tf.test.is_gpu_available():\n",
    "    # GPU\n",
    "    BATCH_SIZE = 20 # Number of examples used in each iteration\n",
    "    EPOCHS = 750 # Number of passes through entire dataset\n",
    "# Hyperparams for CPU training\n",
    "else:\n",
    "    # CPU\n",
    "    BATCH_SIZE = 20\n",
    "    EPOCHS = 10\n",
    "EPOCHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Converting images into tensors and rescaling the pixel values from [0-255] to [0,1].\n",
    "\n",
    "The colornet dataset provides 3 datasets:\n",
    "- **ds-big** with 9600 images\n",
    "- **ds-medium** with 200 images (the pretrained models in the `/floyd/input/colornet/models` folder are trained on this one)\n",
    "- **ds-small** with 20 images (the one used by Emil in the **Full-Version** section of the Blog post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 256, 256, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DS_PATH = '/floyd/input/colornet/ds-small' # ADD path/to/dataset\n",
    "\n",
    "# Get images\n",
    "X = []\n",
    "for filename in os.listdir(DS_PATH):\n",
    "    if os.path.isfile(os.path.join(DS_PATH, filename)):\n",
    "        X.append(img_to_array(load_img(os.path.join(DS_PATH, filename))))\n",
    "                      \n",
    "# Normalization => Converting pixel value from [0-255] to [0,1]                      \n",
    "X = np.array(X, dtype=float)\n",
    "X = X[0:20, :, :, :]\n",
    "Xtrain = 1.0/255*X\n",
    "\n",
    "np.shape(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "Weâ€™ll use an algorithm to change the color channels, from RGB to Lab. L stands for lightness, and a and b for the color spectrums greenâ€“red and blueâ€“yellow.\n",
    "As you can see below, a Lab encoded image has one layer for grayscale and have packed three color layers into two. This means that we can use the original grayscale image in our final prediction. Also, we only have to two channels to predict.\n",
    "\n",
    "\n",
    "![preprocessing](https://blog.floydhub.com/content/images/2018/06/woman_lab_color_space.png)\n",
    "\n",
    "*L/Greyscale to AB - Image from the [Blog](https://blog.floydhub.com/colorizing-b-w-photos-with-neural-networks/)*\n",
    "\n",
    "We have a grayscale layer for input, and we want to predict two color layers, the ab in Lab. To create the final color image weâ€™ll include the L/grayscale image we used for the input, thus, creating a Lab image.\n",
    "\n",
    "![Mapping from B&W to AB](https://blog.floydhub.com/content/images/2018/06/function_lab_color_grids.png)\n",
    "*More formally, we want to learn a mapping from the greyscale to AB - Image from the [Blog](https://blog.floydhub.com/colorizing-b-w-photos-with-neural-networks/)* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from support import create_inception_embedding\n",
    "\n",
    "# Image transformer\n",
    "datagen = ImageDataGenerator(\n",
    "        shear_range=0.1,\n",
    "        zoom_range=0.1,\n",
    "        rotation_range=10,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "def image_a_b_gen(batch_size):\n",
    "    \"\"\"Wrapper on top of ImageDataGenerator which\n",
    "    converts RGB images to B&W, extract the feature using Inception,\n",
    "    and get the LAB from the original image. \n",
    "    \n",
    "    All this information will compose the current batch used \n",
    "    during the training.\"\"\"\n",
    "    for batch in datagen.flow(Xtrain, batch_size=batch_size):\n",
    "        # RGB to B&W\n",
    "        grayscaled_rgb = gray2rgb(rgb2gray(batch))\n",
    "        # Feature Extraction\n",
    "        embed = create_inception_embedding(inception, grayscaled_rgb)\n",
    "        # RGB to LAB\n",
    "        lab_batch = rgb2lab(batch)\n",
    "        X_batch = lab_batch[:,:,:,0]\n",
    "        X_batch = X_batch.reshape(X_batch.shape+(1,))\n",
    "        # Convert LAB value from [-128, 128] to [-1, 1]\n",
    "        Y_batch = lab_batch[:,:,:,1:] / 128\n",
    "        # The new Batch (B&W, Embedding, LAB)\n",
    "        yield ([X_batch, create_inception_embedding(inception, grayscaled_rgb)], Y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "We will implement a model similar to Federico Baldassarreâ€™s [Deep Koalarization: Image Colorization using CNNs and Inception-ResNet-v2](https://arxiv.org/abs/1712.03400). Here are 2 images for the same model:\n",
    "\n",
    "![colornet](https://raw.githubusercontent.com/baldassarreFe/deep-koalarization/master/assets/our_net.png)\n",
    "*Deep Koalarization - Image from [the paper](https://arxiv.org/abs/1712.03400)*\n",
    "\n",
    "![emill's colornet](https://blog.floydhub.com/content/images/2018/06/fusion_layer.png)\n",
    "\n",
    "*Colornet - Image from [the Blog](https://blog.floydhub.com/colorizing-b-w-photos-with-neural-networks/)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.4/xception_weights_tf_dim_ordering_tf_kernels.h5\n",
      "91889664/91884032 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "#Load weights of InceptionResNet model for embedding extraction \n",
    "inception = Xception(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)\n",
    "inception.graph = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 256, 256, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 128, 128, 64) 640         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 128, 128, 64) 256         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 128, 128, 128 73856       batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 128, 128, 128 512         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 64, 64, 128)  147584      batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 64, 64, 128)  512         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 64, 64, 256)  295168      batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 64, 64, 256)  1024        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 32, 32, 256)  590080      batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 32, 32, 256)  1024        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 512)  1180160     batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 32, 512)  2048        conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 512)  2359808     batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 512)  2048        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 256)  1179904     batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 1024, 1000)   0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 32, 32, 256)  1024        conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 32, 32, 1000) 0           repeat_vector_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 1256) 0           batch_normalization_12[0][0]     \n",
      "                                                                 reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 32, 256)  321792      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 32, 32, 256)  1024        conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 32, 32, 128)  295040      batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 64, 64, 128)  0           conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 64)   73792       up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 128, 128, 64) 0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 128, 128, 32) 18464       up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 128, 128, 16) 4624        conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 128, 128, 2)  130         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 256, 256, 2)  0           conv2d_18[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 6,550,514\n",
      "Trainable params: 6,545,778\n",
      "Non-trainable params: 4,736\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# The Model\n",
    "def conv_stack(data, filters, s):\n",
    "    \"\"\"Utility for building conv layer\"\"\"\n",
    "    output = Conv2D(filters, (3, 3), strides=s, activation='relu', padding='same')(data)\n",
    "    return output\n",
    "\n",
    "embed_input = Input(shape=(1000,))\n",
    "\n",
    "#Encoder\n",
    "encoder_input = Input(shape=(256, 256, 1,))\n",
    "encoder_output = conv_stack(encoder_input, 64, 2)\n",
    "encoder_output = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)(encoder_output)\n",
    "encoder_output = conv_stack(encoder_output, 128, 1)\n",
    "encoder_output = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)(encoder_output)\n",
    "encoder_output = conv_stack(encoder_output, 128, 2)\n",
    "encoder_output = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)(encoder_output)\n",
    "encoder_output = conv_stack(encoder_output, 256, 1)\n",
    "encoder_output = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)(encoder_output)\n",
    "encoder_output = conv_stack(encoder_output, 256, 2)\n",
    "encoder_output = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)(encoder_output)\n",
    "encoder_output = conv_stack(encoder_output, 512, 1)\n",
    "encoder_output = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)(encoder_output)\n",
    "encoder_output = conv_stack(encoder_output, 512, 1)\n",
    "encoder_output = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)(encoder_output)\n",
    "encoder_output = conv_stack(encoder_output, 256, 1)\n",
    "encoder_output = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)(encoder_output)\n",
    "#Fusion\n",
    "# y_mid: (None, 256, 28, 28)\n",
    "fusion_output = RepeatVector(32 * 32)(embed_input) \n",
    "fusion_output = Reshape(([32, 32, 1000]))(fusion_output)\n",
    "fusion_output = concatenate([encoder_output, fusion_output], axis=3) \n",
    "fusion_output = Conv2D(256, (1, 1), activation='relu')(fusion_output)\n",
    "fusion_output = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)(fusion_output)\n",
    "\n",
    "\n",
    "\n",
    "#Decoder\n",
    "decoder_output = conv_stack(fusion_output, 128, 1)\n",
    "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "decoder_output = conv_stack(decoder_output, 64, 1)\n",
    "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "decoder_output = conv_stack(decoder_output, 32, 1)\n",
    "decoder_output = conv_stack(decoder_output, 16, 1)\n",
    "decoder_output = Conv2D(2, (2, 2), activation='tanh', padding='same')(decoder_output)\n",
    "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "\n",
    "model = Model(inputs=[encoder_input, embed_input], outputs=decoder_output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Evaluate\n",
    "\n",
    "If you left the default hyperpameters in the Notebook untouched, your training should take approximately: \n",
    "\n",
    "- On CPU machine: 4-5 hours for 250 epochs.\n",
    "- On GPU machine: 50 minutes for 1000 epochs.\n",
    "\n",
    "**Note**: In the dataset you can find different pretrained models that you can use for testing or as a starting point for fine tuning, e.g.: \n",
    "```python\n",
    "# model.load_weights('<path_to_model>')\n",
    "model.load_weights('/floyd/input/colornet/models/color_tensorflow_real_mode_300.h5')\n",
    "```\n",
    "\n",
    "**Emil's advice**\n",
    "\n",
    "It's tricky to get good results. A lot of has to do with how many epochs you train it and which training data you use. *I'd recommend starting with 20-100 images* and **saving at regular intervals**. Once you get a feel for it, you can increase the number of images. Also, use a lot of validation images to understand where it's good and where it struggles.\n",
    "\n",
    "Analyzing the loss data can also be hard. Initially, I noticed that the batch normalization makes the pictures sepia looking. Then it needs additional training to create colors. The loss curve can be misleading because of this.\n",
    "\n",
    "For better results, I'd recommend adding a weighted classification, to favor vibrant colors. If I were to redo it today, I'd experiment with the pix2pixHD GAN structure: https://github.com/NVIDIA/pix2pixHD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 129s - loss: 0.0118\n",
      "Epoch 2/10\n",
      " - 79s - loss: 0.0119\n",
      "Epoch 3/10\n",
      " - 82s - loss: 0.0118\n",
      "Epoch 4/10\n",
      " - 82s - loss: 0.0117\n",
      "Epoch 5/10\n",
      " - 82s - loss: 0.0117\n",
      "Epoch 6/10\n",
      " - 85s - loss: 0.0117\n",
      "Epoch 7/10\n",
      " - 80s - loss: 0.0116\n",
      "Epoch 8/10\n",
      " - 83s - loss: 0.0118\n",
      "Epoch 9/10\n",
      " - 81s - loss: 0.0118\n",
      "Epoch 10/10\n",
      " - 83s - loss: 0.0118\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbf67ca6dd8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train model \n",
    "tensorboard = TensorBoard(log_dir=\"/floyd/home/run\")\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.fit_generator(image_a_b_gen(BATCH_SIZE), \n",
    "                    callbacks=[tensorboard], \n",
    "                    epochs=EPOCHS, steps_per_epoch=1, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval\n",
    "\n",
    "We will use the images in the range [START, END] of the Train for evaluating our model as Emil did during his experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/skimage/util/dtype.py:130: UserWarning: Possible precision loss when converting from float64 to uint8\n",
      "  .format(dtypeobj_in, dtypeobj_out))\n",
      "/usr/local/lib/python3.6/site-packages/skimage/color/colorconv.py:997: UserWarning: Color data out of range: Z < 0 in 4 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "/usr/local/lib/python3.6/site-packages/skimage/color/colorconv.py:997: UserWarning: Color data out of range: Z < 0 in 31 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "/usr/local/lib/python3.6/site-packages/skimage/color/colorconv.py:997: UserWarning: Color data out of range: Z < 0 in 2 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "/usr/local/lib/python3.6/site-packages/skimage/color/colorconv.py:997: UserWarning: Color data out of range: Z < 0 in 9 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "/usr/local/lib/python3.6/site-packages/skimage/color/colorconv.py:997: UserWarning: Color data out of range: Z < 0 in 3 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "/usr/local/lib/python3.6/site-packages/skimage/color/colorconv.py:997: UserWarning: Color data out of range: Z < 0 in 13 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "/usr/local/lib/python3.6/site-packages/skimage/color/colorconv.py:997: UserWarning: Color data out of range: Z < 0 in 17 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "/usr/local/lib/python3.6/site-packages/skimage/color/colorconv.py:997: UserWarning: Color data out of range: Z < 0 in 1 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n"
     ]
    }
   ],
   "source": [
    "# Eval Colorization\n",
    "from support import color_result\n",
    "\n",
    "START = 0\n",
    "END = 100\n",
    "PATH = '/floyd/input/colornet/ds-big/Train/'\n",
    "RESULT = 'result'\n",
    "\n",
    "# It could take some minutes on CPU\n",
    "color_result(PATH, START, END, RESULT, model, inception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfbcfcfe37f44322abae514a65712455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='Show results of colorization', max=99, min=1), Output())â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show results\n",
    "\n",
    "from ipywidgets import interact\n",
    "from ipywidgets import widgets\n",
    "from support import show_img \n",
    "\n",
    "def show_sample(sample_n):\n",
    "    image_path = os.path.join(RESULT, \"img_\"+str(sample_n-1)+\".png\")\n",
    "    img = image.load_img(image_path)\n",
    "    img = image.img_to_array(img)/255\n",
    "    \n",
    "    ax = show_img(img, figsize=(9,9))\n",
    "    ax.set_title(image_path)\n",
    "    \n",
    "interact(show_sample, sample_n=widgets.IntSlider(value=1, min=1, max=END-START-1, description='Show results of colorization'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It's your turn\n",
    "\n",
    "Test out the model you just trained. Run the code Cell below and enter a URL with your pic in the widget below. Have fun!ðŸŽ‰\n",
    "\n",
    "Here's some URL for testing:\n",
    "\n",
    "- (man, colored) http://www.bolsamania.com/cine/wp-content/uploads/2017/03/26-2.jpg\n",
    "- (landscape, colored) https://cdn.pixabay.com/photo/2017/04/07/18/23/landscape-2211587_960_720.jpg\n",
    "- (lion, b&w) https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQTXYpHhz45gaDHPsNulPFotlc72i3MDv_1RoOcQjEQx3sX-dWj\n",
    "\n",
    "**Note**: *You can also consider to use URL of colored pictures, in this way you can fully test the colorization on new images.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23948c531be44d9991fbd2012efba506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='', description='URL', placeholder='Insert URL of a pic'), Button(descriptionâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Testing on url images\n",
    "from ipywidgets import interact_manual\n",
    "from ipywidgets import widgets\n",
    "from support import prediction_from_url\n",
    "\n",
    "def get_prediction(URL):\n",
    "    prediction_from_url(URL, model, inception)\n",
    "\n",
    "interact_manual(get_prediction, URL=widgets.Text(placeholder='Insert URL of a pic'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"models/color_tensorflow_ds_small_{}.h5\".format(EPOCHS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's next\n",
    "\n",
    "Colorizing images is a deeply fascinating problem. It is as much as a scientific problem as artistic one. I wrote this article so you can get up to speed in coloring and continue where I left off. Here are some suggestions to get started:\n",
    "\n",
    "- Implement it with another pre-trained model\n",
    "- A different dataset (you can use **ds-big**)\n",
    "- Enable the network to grow in accuracy with more pictures\n",
    "- Build an amplifier within the RGB color space. Create a similar model to the coloring network, that takes a saturated colored image as input and the correct colored image as output.\n",
    "- Implement a weighted classification\n",
    "- Use a classification neural network as a loss function. Pictures that are classified as fake produce an error. It then decides how much each pixel contributed to the error.\n",
    "- *Apply it to video* (This is a killer AI product). Donâ€™t worry too much about the colorization, but make the switch between images consistent. You could also do something similar for larger images, by tiling smaller ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### That's all folks - don't forget to shutdown your workspace once you're done ðŸ™‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
